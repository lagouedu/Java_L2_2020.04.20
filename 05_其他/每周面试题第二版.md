说明：今后面试题按照知识点分类。

#  每周面试题

## Mysql

- ### 说一下mysql主从复制实现基本原理？

  1、复制是MySQL自带的一项功能，允许服务器将更改从一个服务器的一个实例复制到另一个实例。
  2、主服务器将所有数据和结构更改记录到二进制日志中。
  3、从属服务器从主服务器请求该二进制日志并在本地应用其内容。即通过把主库的binlog传送到从库，从新解析应用到从库。

## 大数据

### hadoop 运行原理

答：hadoop 的主要由两部分组成，HDFS 和 mapreduce，HDFS 就是把数据进行分块存储。 Mapreduce 的原理就是使用 JobTracker 和 TaskTracke r来进行作业的执行。Map 就是将任务展开，reduce 是汇总处理后的结果。简单的来说就是提交一个 jar 包，这个时候需要 mapreduce 来处理。

### 如何确认 hadoop 集群的健康状况

答：有完善的集群监控体系（ganglia，nagios） Hdfs dfsadmin –report Hdfs haadmin –getServiceState nn1

### hadoop 框架怎么来优化

答：可以从很多方面来进行：比如 hdfs，mapreduce，yarn 的 job 调度，hbase，hive 可以优化的有太多地方了，具体要在哪里优化只能看你数据的特点了，根据真实场景来判断。

## spring面试

### spring MVC工作原理

**简单来说：**

客户端发送请求-> 前端控制器 DispatcherServlet 接受客户端请求 -> 找到处理器映射 HandlerMapping 解析请求对应的 Handler-> HandlerAdapter 会根据 Handler 来调用真正的处理器来处理请求，并处理相应的业务逻辑 -> 处理器返回一个模型视图 ModelAndView -> 视图解析器进行解析 -> 返回一个视图对象->前端控制器 DispatcherServlet 渲染数据（Model）->将得到视图对象返回给用户

**流程说明（重要）：**

（1）客户端（浏览器）发送请求，直接请求到 DispatcherServlet。

（2）DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。

（3）解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。

（4）HandlerAdapter 会根据 Handler 来调用真正的处理器开处理请求，并处理相应的业务逻辑。

（5）处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。

（6）ViewResolver 会根据逻辑 View 查找实际的 View。

（7）DispaterServlet 把返回的 Model 传给 View（视图渲染）。

（8）把 View 返回给请求者（浏览器）

## springboot面试

### springboot自启动原理

- 在Spring程序main方法中 添加@SpringBootApplication会自动去maven中读取每个starter中的spring.factoriesspring容器中的bean。
-  Spring Boot 在启动时扫描项目所依赖的JAR包，寻找包含spring.factories文件JAR。 
- 根据spring.factories配置加载AutoConfigure类。 
- 根据@Conditional注解的条件，进行自动配置并将Bean注入Spring Context。



## redis

###  单个Redis实例最多可以存储多少键？哈希表、列表、集合和有序集合最大可以包含多少元素？

Redis最大可以处理2^32键，实践测试每个实例最少可以处理2.5亿键。
每个哈希表、列表、集合和有序集合可以容纳2^32元素。
换句话说，Redis极限容量就是系统可用内存。

### 为什么我的从实例与主实例拥有不同数量键？

如果你使用有生存周期的键，这就是正常现象。这就导致主从实例键的数量不一致原因。
主实例在第一次与从实例同步时生成RDB文件。
RDB文件不包含已经过期的键，但是已经过期的键仍然在内存中。
尽管这些键从逻辑上说已经过期失效，但是还在Redis主实例内存中，他们并不被识别为存在的，当增量或访问这些键时这些键会被回收。尽管从逻辑上说这些键不是数据集一部分，但是INFO和DBSIZE命令结果包含这些信息。
当从实例读取主实例生成的RDB文件时，过期键不会被载入。
为很多键设置过期属性，通常为用户提供了在从实例上存储更少键，但是实际上实例内容没有逻辑区别。

## mongdb

### 副本集成员使用了不同大小的磁盘空间是否正常？

是正常的。
因素包括：不同的oplog大小，不同程度的存储碎片，以及MongoDB的数据文件预分配都可能导致节点之间存储利用率的变化。添加成员的时间不同，则存储使用差异最为明显（译者注：可以理解为先后添加，因此上述存储碎片程度等差异就会比较明显，从而导致影响磁盘占用不同）。

### 我可以重命名副本集吗？

不可以。
您可以使用“ 从MongoDB备份还原副本集”教程中所述的备份和还原过程 来创建具有所需名称的新副本集。可能需要停机时间以确保原始副本集和新副本集之间的奇偶校验。



## 消息机制

### kafka的消息 的 丢失和重复简单叙述下，并说下如何保证消息不丢失？

1、消息发送
         Kafka消息发送有两种方式：同步（sync）和异步（async），默认是同步方式，可通过producer.type属性进行配置。Kafka通过配置request.required.acks属性来确认消息的生产：
0---表示不进行消息接收是否成功的确认；
1---表示当Leader接收成功时确认；
-1---表示Leader和Follower都接收成功时确认；
综上所述，有6种消息生产的情况，下面分情况来分析消息丢失的场景：
（1）acks=0，不和Kafka集群进行消息接收确认，则当网络异常、缓冲区满了等情况时，消息可能丢失；
（2）acks=1、同步模式下，只有Leader确认接收成功后但挂掉了，副本没有同步，数据可能丢失；
2、消息消费
Kafka消息消费有两个consumer接口，Low-level API和High-level API：
Low-level API：消费者自己维护offset等值，可以实现对Kafka的完全控制；
High-level API：封装了对parition和offset的管理，使用简单；
如果使用高级接口High-level API，可能存在一个问题就是当消息消费者从集群中把消息取出来、并提交了新的消息offset值后，还没来得及消费就挂掉了，那么下次再消费时之前没消费成功的消息就“诡异”的消失了；

解决办法：
        针对消息丢失：同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功；异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态；
        针对消息重复：将消息的唯一标识保存到外部介质中，每次消费时判断是否处理过即可。
保证消息不丢失可以参考：https://www.jianshu.com/p/68c173e4c549